{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training pairs: 45797, Validation pairs: 5089\n",
      "Loading GloVe embeddings...\n",
      "Converting texts to embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45797/45797 [00:01<00:00, 34291.08it/s]\n",
      "100%|██████████| 45797/45797 [00:11<00:00, 4143.71it/s]\n",
      "100%|██████████| 5089/5089 [00:00<00:00, 33711.03it/s]\n",
      "100%|██████████| 5089/5089 [00:01<00:00, 4100.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Baseline: 100%|██████████| 1432/1432 [00:01<00:00, 971.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline - Training Loss: 0.6911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training CTPE: 100%|██████████| 1432/1432 [00:02<00:00, 614.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTPE - Training Loss: 0.6900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Baseline: 100%|██████████| 160/160 [00:00<00:00, 2501.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs Shape: torch.Size([1])\n",
      "Labels Shape: torch.Size([1])\n",
      "Baseline - Validation Loss: 0.6888, Accuracy: 0.7088, F1 Score: 0.7315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating CTPE: 100%|██████████| 160/160 [00:00<00:00, 1805.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTPE - Validation Loss: 0.6763, Accuracy: 0.6310, F1 Score: 0.6945\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Baseline: 100%|██████████| 1432/1432 [00:01<00:00, 1323.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline - Training Loss: 0.6872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training CTPE: 100%|██████████| 1432/1432 [00:02<00:00, 572.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTPE - Training Loss: 0.6443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Baseline: 100%|██████████| 160/160 [00:00<00:00, 2435.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs Shape: torch.Size([1])\n",
      "Labels Shape: torch.Size([1])\n",
      "Baseline - Validation Loss: 0.6847, Accuracy: 0.7106, F1 Score: 0.7371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating CTPE: 100%|██████████| 160/160 [00:00<00:00, 1802.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTPE - Validation Loss: 0.6093, Accuracy: 0.6854, F1 Score: 0.7210\n",
      "\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Baseline: 100%|██████████| 1432/1432 [00:01<00:00, 1311.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline - Training Loss: 0.6834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training CTPE: 100%|██████████| 1432/1432 [00:02<00:00, 559.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTPE - Training Loss: 0.5993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Baseline: 100%|██████████| 160/160 [00:00<00:00, 2437.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs Shape: torch.Size([1])\n",
      "Labels Shape: torch.Size([1])\n",
      "Baseline - Validation Loss: 0.6807, Accuracy: 0.7300, F1 Score: 0.7275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating CTPE: 100%|██████████| 160/160 [00:00<00:00, 1528.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTPE - Validation Loss: 0.5787, Accuracy: 0.7031, F1 Score: 0.7204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Baseline Scores: 100%|██████████| 160/160 [00:00<00:00, 2756.96it/s]\n",
      "Extracting CTPE Scores: 100%|██████████| 160/160 [00:00<00:00, 1744.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Validation Metrics:\n",
      "Baseline (Bi-Encoder) - Accuracy: 0.7300, F1 Score: 0.7275\n",
      "CTPE (Cross-Encoder) - Accuracy: 0.7031, F1 Score: 0.7204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch  # Not strictly necessary for this model but kept for consistency\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_curve, auc\n",
    "from sklearn.preprocessing import normalize\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gensim.downloader as api  # For loading GloVe embeddings\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Data Preparation\n",
    "# -----------------------------\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"/data2/home/rohitsunil/termreport/arxiv_new.csv\")\n",
    "df = df.rename(columns={'summary': 'abstract'})\n",
    "\n",
    "# Create positive pairs\n",
    "positive_pairs = df[['title', 'abstract']].copy()\n",
    "positive_pairs['label'] = 1\n",
    "\n",
    "# Create negative pairs by shuffling abstracts\n",
    "shuffled_abstracts = df['abstract'].sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "negative_pairs = pd.DataFrame({\n",
    "    'title': df['title'],\n",
    "    'abstract': shuffled_abstracts,\n",
    "    'label': 0\n",
    "})\n",
    "\n",
    "# Combine and shuffle all pairs\n",
    "all_pairs = pd.concat([positive_pairs, negative_pairs], ignore_index=True)\n",
    "all_pairs = all_pairs.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_pairs, val_pairs = train_test_split(all_pairs, test_size=0.1, random_state=42)\n",
    "\n",
    "print(f\"Training pairs: {len(train_pairs)}, Validation pairs: {len(val_pairs)}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Load Pre-trained GloVe Embeddings\n",
    "# -----------------------------\n",
    "\n",
    "# Download and load GloVe embeddings using Gensim\n",
    "print(\"Loading GloVe embeddings...\")\n",
    "glove = api.load(\"glove-wiki-gigaword-100\")  # This may take a while if not already downloaded\n",
    "\n",
    "embedding_dim = 100  # Dimension of GloVe embeddings\n",
    "\n",
    "# Function to convert text to embedding by averaging word embeddings\n",
    "def text_to_embedding(text, embedding_model, embedding_dim):\n",
    "    \"\"\"\n",
    "    Converts a given text to an embedding by averaging its word embeddings.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "        embedding_model: Pre-trained embedding model (GloVe).\n",
    "        embedding_dim (int): Dimension of the embeddings.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Averaged embedding vector.\n",
    "    \"\"\"\n",
    "    words = text.lower().split()\n",
    "    valid_embeddings = [embedding_model[word] for word in words if word in embedding_model]\n",
    "    if valid_embeddings:\n",
    "        return np.mean(valid_embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(embedding_dim)\n",
    "\n",
    "# Apply the function to all titles and abstracts in training and validation sets\n",
    "print(\"Converting texts to embeddings...\")\n",
    "\n",
    "train_titles_emb = np.array([text_to_embedding(title, glove, embedding_dim) for title in tqdm(train_pairs['title'])])\n",
    "train_abstracts_emb = np.array([text_to_embedding(abstract, glove, embedding_dim) for abstract in tqdm(train_pairs['abstract'])])\n",
    "train_labels = train_pairs['label'].values\n",
    "\n",
    "val_titles_emb = np.array([text_to_embedding(title, glove, embedding_dim) for title in tqdm(val_pairs['title'])])\n",
    "val_abstracts_emb = np.array([text_to_embedding(abstract, glove, embedding_dim) for abstract in tqdm(val_pairs['abstract'])])\n",
    "val_labels = val_pairs['label'].values\n",
    "\n",
    "# Normalize the embeddings to unit vectors\n",
    "train_titles_emb = normalize(train_titles_emb)\n",
    "train_abstracts_emb = normalize(train_abstracts_emb)\n",
    "val_titles_emb = normalize(val_titles_emb)\n",
    "val_abstracts_emb = normalize(val_abstracts_emb)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Feature Extraction\n",
    "# -----------------------------\n",
    "\n",
    "# Baseline (Bi-Encoder): Compute cosine similarity between title and abstract embeddings\n",
    "# CTPE (Cross-Encoder): Concatenate title and abstract embeddings and use as input features\n",
    "\n",
    "# For CTPE, we'll concatenate title and abstract embeddings\n",
    "train_features_ctpe = np.concatenate([train_titles_emb, train_abstracts_emb], axis=1)\n",
    "val_features_ctpe = np.concatenate([val_titles_emb, val_abstracts_emb], axis=1)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Define Datasets and DataLoaders\n",
    "# -----------------------------\n",
    "\n",
    "# Since we're using precomputed embeddings, we can use custom datasets\n",
    "\n",
    "class BaselineDataset(Dataset):\n",
    "    def __init__(self, titles, abstracts, labels):\n",
    "        self.titles = torch.tensor(titles, dtype=torch.float32)\n",
    "        self.abstracts = torch.tensor(abstracts, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'title': self.titles[idx],\n",
    "            'abstract': self.abstracts[idx],\n",
    "            'label': self.labels[idx]\n",
    "        }\n",
    "\n",
    "class CTPEDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'features': self.features[idx],\n",
    "            'label': self.labels[idx]\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset_baseline = BaselineDataset(train_titles_emb, train_abstracts_emb, train_labels)\n",
    "val_dataset_baseline = BaselineDataset(val_titles_emb, val_abstracts_emb, val_labels)\n",
    "\n",
    "train_dataset_ctpe = CTPEDataset(train_features_ctpe, train_labels)\n",
    "val_dataset_ctpe = CTPEDataset(val_features_ctpe, val_labels)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "\n",
    "train_loader_baseline = DataLoader(train_dataset_baseline, batch_size=batch_size, shuffle=True)\n",
    "val_loader_baseline = DataLoader(val_dataset_baseline, batch_size=batch_size)\n",
    "\n",
    "train_loader_ctpe = DataLoader(train_dataset_ctpe, batch_size=batch_size, shuffle=True)\n",
    "val_loader_ctpe = DataLoader(val_dataset_ctpe, batch_size=batch_size)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Define the Models\n",
    "# -----------------------------\n",
    "\n",
    "# Baseline (Bi-Encoder): Compute cosine similarity and use contrastive loss\n",
    "# Since we're using Logistic Regression and GloVe, we'll implement a simple neural network\n",
    "\n",
    "class BaselineNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(BaselineNN, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, 1)  # Combine title and abstract embeddings\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, title, abstract):\n",
    "        # Compute element-wise product as interaction feature\n",
    "        interaction = title * abstract\n",
    "        out = self.fc(interaction)\n",
    "        out = self.sigmoid(out)\n",
    "        return out.squeeze(dim=1)  # Corrected squeeze operation\n",
    "\n",
    "# CTPE (Cross-Encoder): Use a neural network classifier on concatenated embeddings\n",
    "\n",
    "class CTPE_NN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(CTPE_NN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, features):\n",
    "        out = self.fc1(features)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out.squeeze(dim=1)  # Ensure output shape is (batch_size,)\n",
    "\n",
    "# Initialize models\n",
    "baseline_model = BaselineNN(input_dim=embedding_dim).to('cpu')  # No GPU needed\n",
    "ctpe_model = CTPE_NN(input_dim=embedding_dim*2).to('cpu')    # No GPU needed\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Define Loss Functions and Optimizers\n",
    "# -----------------------------\n",
    "\n",
    "# Binary Cross-Entropy Loss\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Optimizers\n",
    "optimizer_baseline = optim.Adam(baseline_model.parameters(), lr=1e-3)\n",
    "optimizer_ctpe = optim.Adam(ctpe_model.parameters(), lr=1e-3)\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Training and Evaluation Functions\n",
    "# -----------------------------\n",
    "\n",
    "def train_baseline(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(dataloader, desc='Training Baseline'):\n",
    "        titles = batch['title']\n",
    "        abstracts = batch['abstract']\n",
    "        labels = batch['label']\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(titles, abstracts)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n",
    "\n",
    "def evaluate_baseline(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Evaluating Baseline'):\n",
    "            titles = batch['title']\n",
    "            abstracts = batch['abstract']\n",
    "            labels = batch['label']\n",
    "            \n",
    "            outputs = model(titles, abstracts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = (outputs >= 0.5).float()\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_preds.extend(preds.numpy())\n",
    "    \n",
    "    # Print shapes for verification\n",
    "    print(f\"Outputs Shape: {outputs.shape}\")\n",
    "    print(f\"Labels Shape: {labels.shape}\")\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    return avg_loss, accuracy, f1\n",
    "\n",
    "def train_ctpe(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(dataloader, desc='Training CTPE'):\n",
    "        features = batch['features']\n",
    "        labels = batch['label']\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n",
    "\n",
    "def evaluate_ctpe(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Evaluating CTPE'):\n",
    "            features = batch['features']\n",
    "            labels = batch['label']\n",
    "            \n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = (outputs >= 0.5).float()\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_preds.extend(preds.numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    return avg_loss, accuracy, f1\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Training and Evaluation Loop\n",
    "# -----------------------------\n",
    "\n",
    "num_epochs = 3  # Adjust as needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    # Train Baseline\n",
    "    train_loss_baseline = train_baseline(baseline_model, train_loader_baseline, optimizer_baseline, criterion)\n",
    "    print(f'Baseline - Training Loss: {train_loss_baseline:.4f}')\n",
    "    \n",
    "    # Train CTPE\n",
    "    train_loss_ctpe = train_ctpe(ctpe_model, train_loader_ctpe, optimizer_ctpe, criterion)\n",
    "    print(f'CTPE - Training Loss: {train_loss_ctpe:.4f}')\n",
    "    \n",
    "    # Evaluate Baseline\n",
    "    val_loss_baseline, val_acc_baseline, val_f1_baseline = evaluate_baseline(baseline_model, val_loader_baseline, criterion)\n",
    "    print(f'Baseline - Validation Loss: {val_loss_baseline:.4f}, Accuracy: {val_acc_baseline:.4f}, F1 Score: {val_f1_baseline:.4f}')\n",
    "    \n",
    "    # Evaluate CTPE\n",
    "    val_loss_ctpe, val_acc_ctpe, val_f1_ctpe = evaluate_ctpe(ctpe_model, val_loader_ctpe, criterion)\n",
    "    print(f'CTPE - Validation Loss: {val_loss_ctpe:.4f}, Accuracy: {val_acc_ctpe:.4f}, F1 Score: {val_f1_ctpe:.4f}')\n",
    "\n",
    "\n",
    "\n",
    "# Baseline\n",
    "baseline_model.eval()\n",
    "all_labels_baseline = []\n",
    "all_scores_baseline = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader_baseline, desc='Extracting Baseline Scores'):\n",
    "        titles = batch['title']\n",
    "        abstracts = batch['abstract']\n",
    "        labels = batch['label'].numpy()\n",
    "        outputs = baseline_model(titles, abstracts)\n",
    "        all_labels_baseline.extend(labels)\n",
    "        all_scores_baseline.extend(outputs.numpy())\n",
    "\n",
    "# CTPE\n",
    "ctpe_model.eval()\n",
    "all_labels_ctpe = []\n",
    "all_scores_ctpe = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader_ctpe, desc='Extracting CTPE Scores'):\n",
    "        features = batch['features']\n",
    "        labels = batch['label'].numpy()\n",
    "        outputs = ctpe_model(features)\n",
    "        all_labels_ctpe.extend(labels)\n",
    "        all_scores_ctpe.extend(outputs.numpy())\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nFinal Validation Metrics:\")\n",
    "print(f\"Baseline (Bi-Encoder) - Accuracy: {accuracy_score(all_labels_baseline, (np.array(all_scores_baseline) >= 0.5).astype(float)):.4f}, F1 Score: {f1_score(all_labels_baseline, (np.array(all_scores_baseline) >= 0.5).astype(float)):.4f}\")\n",
    "print(f\"CTPE (Cross-Encoder) - Accuracy: {accuracy_score(all_labels_ctpe, (np.array(all_scores_ctpe) >= 0.5).astype(float)):.4f}, F1 Score: {f1_score(all_labels_ctpe, (np.array(all_scores_ctpe) >= 0.5).astype(float)):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
