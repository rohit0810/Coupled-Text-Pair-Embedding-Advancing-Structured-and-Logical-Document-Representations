{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 22:34:14.440335: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-28 22:34:14.443157: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-28 22:34:14.478484: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-28 22:34:14.478505: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-28 22:34:14.478536: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-28 22:34:14.486752: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-28 22:34:15.289869: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/home/rohitsunil/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf7969fca2144cc8b61f912147299cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40454abacbd747b094d2000aa0094ced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fc155b4c9ab4616b21e02333d056690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a14ab4782c734c6b80aaf1ecb793a596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/home/rohitsunil/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75b4b61a19174bec98d977ce6488e3ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/home/rohitsunil/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:399: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline - Training pairs: 45797, Validation pairs: 5089\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"/data2/home/rohitsunil/termreport/arxiv_new.csv\")\n",
    "df = df.rename(columns={'summary': 'abstract'})\n",
    "\n",
    "# Load the pretrained tokenizer and model for Baseline (Bi-Encoder)\n",
    "tokenizer_baseline = AutoTokenizer.from_pretrained('microsoft/MiniLM-L12-H384-uncased')\n",
    "minilm_baseline = AutoModel.from_pretrained('microsoft/MiniLM-L12-H384-uncased')\n",
    "minilm_baseline.to(device)\n",
    "\n",
    "# Load the pretrained tokenizer and model for CTPE (Cross-Encoder)\n",
    "tokenizer_ctpe = AutoTokenizer.from_pretrained('microsoft/MiniLM-L12-H384-uncased')\n",
    "minilm_ctpe = AutoModel.from_pretrained('microsoft/MiniLM-L12-H384-uncased')\n",
    "minilm_ctpe.to(device)\n",
    "\n",
    "# Maximum sequence length for MiniLM\n",
    "max_length = 128  # Adjust as needed\n",
    "\n",
    "# Function to tokenize and encode texts for Baseline (Bi-Encoder)\n",
    "def tokenize_texts_baseline(texts):\n",
    "    return tokenizer_baseline(\n",
    "        texts.tolist(),\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt'  # Return PyTorch tensors\n",
    "    )\n",
    "\n",
    "# Function to tokenize and encode texts for CTPE (Cross-Encoder)\n",
    "def tokenize_texts_ctpe(titles, abstracts):\n",
    "    # MiniLM does not support token_type_ids, so we'll concatenate titles and abstracts with [SEP]\n",
    "    concatenated_texts = [f\"{title} [SEP] {abstract}\" for title, abstract in zip(titles, abstracts)]\n",
    "    return tokenizer_ctpe(\n",
    "        concatenated_texts,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "# Tokenize titles and abstracts for Baseline\n",
    "title_encodings_baseline = tokenize_texts_baseline(df['title'])\n",
    "abstract_encodings_baseline = tokenize_texts_baseline(df['abstract'])\n",
    "\n",
    "# Create positive pairs for Baseline\n",
    "positive_pairs_baseline = pd.DataFrame({\n",
    "    'title_input_ids': list(title_encodings_baseline['input_ids']),\n",
    "    'title_attention_mask': list(title_encodings_baseline['attention_mask']),\n",
    "    'abstract_input_ids': list(abstract_encodings_baseline['input_ids']),\n",
    "    'abstract_attention_mask': list(abstract_encodings_baseline['attention_mask']),\n",
    "    'label': 1\n",
    "})\n",
    "\n",
    "# Create negative pairs by shuffling abstracts for Baseline\n",
    "shuffled_abstracts = df['abstract'].sample(frac=1).reset_index(drop=True)\n",
    "shuffled_abstract_encodings_baseline = tokenize_texts_baseline(shuffled_abstracts)\n",
    "\n",
    "negative_pairs_baseline = pd.DataFrame({\n",
    "    'title_input_ids': list(title_encodings_baseline['input_ids']),\n",
    "    'title_attention_mask': list(title_encodings_baseline['attention_mask']),\n",
    "    'abstract_input_ids': list(shuffled_abstract_encodings_baseline['input_ids']),\n",
    "    'abstract_attention_mask': list(shuffled_abstract_encodings_baseline['attention_mask']),\n",
    "    'label': 0\n",
    "})\n",
    "\n",
    "# Combine and shuffle Baseline pairs\n",
    "all_pairs_baseline = pd.concat([positive_pairs_baseline, negative_pairs_baseline], ignore_index=True)\n",
    "all_pairs_baseline = all_pairs_baseline.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Split into training and validation sets for Baseline\n",
    "train_pairs_baseline, val_pairs_baseline = train_test_split(all_pairs_baseline, test_size=0.1, random_state=42)\n",
    "\n",
    "print(f\"Baseline - Training pairs: {len(train_pairs_baseline)}, Validation pairs: {len(val_pairs_baseline)}\")\n",
    "\n",
    "# Prepare data for CTPE (Cross-Encoder)\n",
    "positive_pairs_ctpe = pd.DataFrame({\n",
    "    'title': df['title'],\n",
    "    'abstract': df['abstract'],\n",
    "    'label': 1\n",
    "})\n",
    "\n",
    "negative_pairs_ctpe = pd.DataFrame({\n",
    "    'title': df['title'],\n",
    "    'abstract': shuffled_abstracts,\n",
    "    'label': 0\n",
    "})\n",
    "\n",
    "\n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTPE - Training pairs: 45797, Validation pairs: 5089\n"
     ]
    }
   ],
   "source": [
    "# Combine and shuffle CTPE pairs\n",
    "all_pairs_ctpe = pd.concat([positive_pairs_ctpe, negative_pairs_ctpe], ignore_index=True)\n",
    "all_pairs_ctpe = all_pairs_ctpe.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Split into training and validation sets for CTPE\n",
    "train_pairs_ctpe, val_pairs_ctpe = train_test_split(all_pairs_ctpe, test_size=0.1, random_state=42)\n",
    "\n",
    "print(f\"CTPE - Training pairs: {len(train_pairs_ctpe)}, Validation pairs: {len(val_pairs_ctpe)}\")\n",
    "\n",
    "# Dataset for Baseline (Bi-Encoder)\n",
    "class BaselinePairDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.title_input_ids = pairs['title_input_ids'].values\n",
    "        self.title_attention_mask = pairs['title_attention_mask'].values\n",
    "        self.abstract_input_ids = pairs['abstract_input_ids'].values\n",
    "        self.abstract_attention_mask = pairs['abstract_attention_mask'].values\n",
    "        self.labels = pairs['label'].values.astype(np.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'title_input_ids': torch.tensor(self.title_input_ids[idx], dtype=torch.long),\n",
    "            'title_attention_mask': torch.tensor(self.title_attention_mask[idx], dtype=torch.long),\n",
    "            'abstract_input_ids': torch.tensor(self.abstract_input_ids[idx], dtype=torch.long),\n",
    "            'abstract_attention_mask': torch.tensor(self.abstract_attention_mask[idx], dtype=torch.long),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# Dataset for CTPE (Cross-Encoder)\n",
    "class CTPEDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.titles = pairs['title'].values\n",
    "        self.abstracts = pairs['abstract'].values\n",
    "        self.labels = pairs['label'].values.astype(np.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'title': self.titles[idx],\n",
    "            'abstract': self.abstracts[idx],\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset_baseline = BaselinePairDataset(train_pairs_baseline)\n",
    "val_dataset_baseline = BaselinePairDataset(val_pairs_baseline)\n",
    "\n",
    "train_dataset_ctpe = CTPEDataset(train_pairs_ctpe)\n",
    "val_dataset_ctpe = CTPEDataset(val_pairs_ctpe)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32  # Adjust as needed based on your GPU capacity\n",
    "\n",
    "train_loader_baseline = DataLoader(train_dataset_baseline, batch_size=batch_size, shuffle=True)\n",
    "val_loader_baseline = DataLoader(val_dataset_baseline, batch_size=batch_size)\n",
    "\n",
    "train_loader_ctpe = DataLoader(train_dataset_ctpe, batch_size=batch_size, shuffle=True)\n",
    "val_loader_ctpe = DataLoader(val_dataset_ctpe, batch_size=batch_size)\n",
    "\n",
    "# Baseline (Bi-Encoder) Model\n",
    "class BaselineDocumentSimilarityModel(nn.Module):\n",
    "    def __init__(self, minilm_model):\n",
    "        super(BaselineDocumentSimilarityModel, self).__init__()\n",
    "        self.minilm = minilm_model\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.cosine_similarity = nn.CosineSimilarity(dim=1)\n",
    "    \n",
    "    def forward(self, title_input_ids, title_attention_mask, abstract_input_ids, abstract_attention_mask):\n",
    "        # Process titles\n",
    "        title_outputs = self.minilm(\n",
    "            input_ids=title_input_ids,\n",
    "            attention_mask=title_attention_mask\n",
    "        )\n",
    "        title_pooled_output = title_outputs.last_hidden_state[:, 0, :]  # Shape: (batch_size, hidden_size)\n",
    "        \n",
    "        # Process abstracts\n",
    "        abstract_outputs = self.minilm(\n",
    "            input_ids=abstract_input_ids,\n",
    "            attention_mask=abstract_attention_mask\n",
    "        )\n",
    "        abstract_pooled_output = abstract_outputs.last_hidden_state[:, 0, :]  # Shape: (batch_size, hidden_size)\n",
    "        \n",
    "        # Apply dropout\n",
    "        title_vector = self.dropout(title_pooled_output)\n",
    "        abstract_vector = self.dropout(abstract_pooled_output)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        similarity = self.cosine_similarity(title_vector, abstract_vector)\n",
    "        return similarity\n",
    "\n",
    "# CTPE (Cross-Encoder) Model\n",
    "class CTPEDocumentSimilarityModel(nn.Module):\n",
    "    def __init__(self, minilm_model):\n",
    "        super(CTPEDocumentSimilarityModel, self).__init__()\n",
    "        self.minilm = minilm_model\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.minilm.config.hidden_size, 1)  # Output a single score\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.minilm(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]  # Using [CLS] token representation\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output).squeeze(-1)  # Shape: (batch_size)\n",
    "        return logits\n",
    "\n",
    "# Initialize the models\n",
    "model_baseline = BaselineDocumentSimilarityModel(minilm_baseline)\n",
    "model_baseline.to(device)\n",
    "\n",
    "model_ctpe = CTPEDocumentSimilarityModel(minilm_ctpe)\n",
    "model_ctpe.to(device)\n",
    "\n",
    "# Define the loss functions\n",
    "def contrastive_loss(similarity, label):\n",
    "    \"\"\"\n",
    "    Contrastive loss function for Baseline (Bi-Encoder) model.\n",
    "    Args:\n",
    "        similarity: Cosine similarity scores between title and abstract embeddings.\n",
    "        label: Ground truth labels (1 for positive pairs, 0 for negative pairs).\n",
    "    Returns:\n",
    "        Computed contrastive loss.\n",
    "    \"\"\"\n",
    "    margin = 0.1  # Margin value\n",
    "    loss = torch.mean(\n",
    "        label * torch.pow(1 - similarity, 2) +\n",
    "        (1 - label) * torch.pow(torch.clamp(similarity - margin, min=0.0), 2)\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "def binary_cross_entropy_loss(logits, labels):\n",
    "    \"\"\"\n",
    "    Binary Cross-Entropy Loss with logits for CTPE (Cross-Encoder) model.\n",
    "    Args:\n",
    "        logits: Raw output scores from the classifier.\n",
    "        labels: Ground truth labels (1 for positive pairs, 0 for negative pairs).\n",
    "    Returns:\n",
    "        Computed binary cross-entropy loss.\n",
    "    \"\"\"\n",
    "    loss_fct = nn.BCEWithLogitsLoss()\n",
    "    return loss_fct(logits, labels)\n",
    "\n",
    "# Define the optimizers\n",
    "optimizer_baseline = torch.optim.AdamW(model_baseline.parameters(), lr=2e-5)\n",
    "optimizer_ctpe = torch.optim.AdamW(model_ctpe.parameters(), lr=2e-5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Baseline:   0%|          | 0/1432 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_230201/2204557032.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'title_input_ids': torch.tensor(self.title_input_ids[idx], dtype=torch.long),\n",
      "/tmp/ipykernel_230201/2204557032.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'title_attention_mask': torch.tensor(self.title_attention_mask[idx], dtype=torch.long),\n",
      "/tmp/ipykernel_230201/2204557032.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'abstract_input_ids': torch.tensor(self.abstract_input_ids[idx], dtype=torch.long),\n",
      "/tmp/ipykernel_230201/2204557032.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'abstract_attention_mask': torch.tensor(self.abstract_attention_mask[idx], dtype=torch.long),\n",
      "Training Baseline: 100%|██████████| 1432/1432 [02:49<00:00,  8.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline - Training Loss: 0.1781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training CTPE: 100%|██████████| 1432/1432 [01:45<00:00, 13.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTPE - Training Loss: 0.0179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Baseline: 100%|██████████| 160/160 [00:06<00:00, 25.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline - Validation Loss: 0.1489, Accuracy: 0.7326, F1 Score: 0.7706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating CTPE: 100%|██████████| 160/160 [00:05<00:00, 31.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTPE - Validation Loss: 0.0312, Accuracy: 0.9896, F1 Score: 0.9896\n",
      "\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Baseline:   0%|          | 0/1432 [00:00<?, ?it/s]/tmp/ipykernel_230201/2204557032.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'title_input_ids': torch.tensor(self.title_input_ids[idx], dtype=torch.long),\n",
      "/tmp/ipykernel_230201/2204557032.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'title_attention_mask': torch.tensor(self.title_attention_mask[idx], dtype=torch.long),\n",
      "/tmp/ipykernel_230201/2204557032.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'abstract_input_ids': torch.tensor(self.abstract_input_ids[idx], dtype=torch.long),\n",
      "/tmp/ipykernel_230201/2204557032.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'abstract_attention_mask': torch.tensor(self.abstract_attention_mask[idx], dtype=torch.long),\n",
      "Training Baseline: 100%|██████████| 1432/1432 [02:49<00:00,  8.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline - Training Loss: 0.1263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training CTPE: 100%|██████████| 1432/1432 [01:45<00:00, 13.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTPE - Training Loss: 0.0141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Baseline: 100%|██████████| 160/160 [00:06<00:00, 25.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline - Validation Loss: 0.1067, Accuracy: 0.8112, F1 Score: 0.8313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating CTPE: 100%|██████████| 160/160 [00:04<00:00, 32.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTPE - Validation Loss: 0.0284, Accuracy: 0.9914, F1 Score: 0.9914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Accuracy Evaluation Baseline:   0%|          | 0/160 [00:00<?, ?it/s]/tmp/ipykernel_230201/2204557032.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'title_input_ids': torch.tensor(self.title_input_ids[idx], dtype=torch.long),\n",
      "/tmp/ipykernel_230201/2204557032.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'title_attention_mask': torch.tensor(self.title_attention_mask[idx], dtype=torch.long),\n",
      "/tmp/ipykernel_230201/2204557032.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'abstract_input_ids': torch.tensor(self.abstract_input_ids[idx], dtype=torch.long),\n",
      "/tmp/ipykernel_230201/2204557032.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'abstract_attention_mask': torch.tensor(self.abstract_attention_mask[idx], dtype=torch.long),\n",
      "Accuracy Evaluation Baseline: 100%|██████████| 160/160 [00:06<00:00, 25.07it/s]\n",
      "Accuracy Evaluation CTPE: 100%|██████████| 160/160 [00:04<00:00, 32.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Validation Accuracy:\n",
      "Baseline (Bi-Encoder): 0.8112\n",
      "CTPE (Cross-Encoder): 0.9914\n",
      "\n",
      "Extracting Embeddings for Baseline (Bi-Encoder)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Baseline Embeddings: 100%|██████████| 160/160 [00:06<00:00, 24.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title Embeddings Shape (Baseline): (5089, 384)\n",
      "Abstract Embeddings Shape (Baseline): (5089, 384)\n",
      "\n",
      "Extracting Embeddings for CTPE (Cross-Encoder)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting CTPE Embeddings: 100%|██████████| 160/160 [00:04<00:00, 32.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair Embeddings Shape (CTPE): (5089, 384)\n",
      "\n",
      "Using CTPE logits as similarity scores...\n",
      "Similarity Scores (CTPE): [[0.3580209  0.40198722 0.47324777 ... 0.35267502 0.37274146 0.44419846]\n",
      " [0.3628221  0.40083635 0.47095144 ... 0.3529054  0.38161293 0.44883114]\n",
      " [0.60188454 0.6139684  0.4991282  ... 0.6318804  0.57787585 0.546747  ]\n",
      " [0.5537811  0.5808418  0.50977176 ... 0.60095686 0.5214416  0.5432543 ]\n",
      " [0.59874874 0.61812985 0.5098833  ... 0.63340414 0.6241622  0.5477605 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training Parameters\n",
    "num_epochs = 2  # Adjust as needed based on your dataset and resources\n",
    "\n",
    "# Training Loop for Baseline (Bi-Encoder)\n",
    "def train_baseline(model, dataloader, optimizer):\n",
    "    \"\"\"\n",
    "    Training loop for the Baseline (Bi-Encoder) model.\n",
    "    Args:\n",
    "        model: BaselineDocumentSimilarityModel instance.\n",
    "        dataloader: DataLoader for the training data.\n",
    "        optimizer: Optimizer for the model.\n",
    "    Returns:\n",
    "        Average training loss for the epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(dataloader, desc='Training Baseline'):\n",
    "        # Move data to GPU\n",
    "        title_input_ids = batch['title_input_ids'].to(device)\n",
    "        title_attention_mask = batch['title_attention_mask'].to(device)\n",
    "        abstract_input_ids = batch['abstract_input_ids'].to(device)\n",
    "        abstract_attention_mask = batch['abstract_attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        similarities = model(\n",
    "            title_input_ids,\n",
    "            title_attention_mask,\n",
    "            abstract_input_ids,\n",
    "            abstract_attention_mask\n",
    "        )\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = contrastive_loss(similarities, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n",
    "\n",
    "# Training Loop for CTPE (Cross-Encoder)\n",
    "def train_ctpe(model, dataloader, optimizer):\n",
    "    \"\"\"\n",
    "    Training loop for the CTPE (Cross-Encoder) model.\n",
    "    Args:\n",
    "        model: CTPEDocumentSimilarityModel instance.\n",
    "        dataloader: DataLoader for the training data.\n",
    "        optimizer: Optimizer for the model.\n",
    "    Returns:\n",
    "        Average training loss for the epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(dataloader, desc='Training CTPE'):\n",
    "        # Move data to GPU\n",
    "        titles = batch['title']\n",
    "        abstracts = batch['abstract']\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        # Tokenize the concatenated title and abstract\n",
    "        encodings = tokenize_texts_ctpe(titles, abstracts)\n",
    "        input_ids = encodings['input_ids'].to(device)\n",
    "        attention_mask = encodings['attention_mask'].to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = binary_cross_entropy_loss(logits, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n",
    "\n",
    "# Evaluation Function for Baseline (Bi-Encoder)\n",
    "def evaluate_baseline(model, dataloader):\n",
    "    \"\"\"\n",
    "    Evaluation loop for the Baseline (Bi-Encoder) model.\n",
    "    Args:\n",
    "        model: BaselineDocumentSimilarityModel instance.\n",
    "        dataloader: DataLoader for the validation data.\n",
    "    Returns:\n",
    "        Tuple containing average validation loss, accuracy, and F1 score.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Evaluating Baseline'):\n",
    "            # Move data to GPU\n",
    "            title_input_ids = batch['title_input_ids'].to(device)\n",
    "            title_attention_mask = batch['title_attention_mask'].to(device)\n",
    "            abstract_input_ids = batch['abstract_input_ids'].to(device)\n",
    "            abstract_attention_mask = batch['abstract_attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            similarities = model(\n",
    "                title_input_ids,\n",
    "                title_attention_mask,\n",
    "                abstract_input_ids,\n",
    "                abstract_attention_mask\n",
    "            )\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = contrastive_loss(similarities, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Compute predictions\n",
    "            preds = (similarities >= 0.5).float()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(preds.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    f1 = f1_score(all_labels, all_predictions)\n",
    "    return avg_loss, accuracy, f1\n",
    "\n",
    "# Evaluation Function for CTPE (Cross-Encoder)\n",
    "def evaluate_ctpe(model, dataloader):\n",
    "    \"\"\"\n",
    "    Evaluation loop for the CTPE (Cross-Encoder) model.\n",
    "    Args:\n",
    "        model: CTPEDocumentSimilarityModel instance.\n",
    "        dataloader: DataLoader for the validation data.\n",
    "    Returns:\n",
    "        Tuple containing average validation loss, accuracy, and F1 score.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Evaluating CTPE'):\n",
    "            # Move data to GPU\n",
    "            titles = batch['title']\n",
    "            abstracts = batch['abstract']\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Tokenize the concatenated title and abstract\n",
    "            encodings = tokenize_texts_ctpe(titles, abstracts)\n",
    "            input_ids = encodings['input_ids'].to(device)\n",
    "            attention_mask = encodings['attention_mask'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = binary_cross_entropy_loss(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Compute predictions\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs >= 0.5).float()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(preds.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    f1 = f1_score(all_labels, all_predictions)\n",
    "    return avg_loss, accuracy, f1\n",
    "\n",
    "# Training and Evaluation Loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    # Train Baseline\n",
    "    train_loss_baseline = train_baseline(model_baseline, train_loader_baseline, optimizer_baseline)\n",
    "    print(f'Baseline - Training Loss: {train_loss_baseline:.4f}')\n",
    "    \n",
    "    # Train CTPE\n",
    "    train_loss_ctpe = train_ctpe(model_ctpe, train_loader_ctpe, optimizer_ctpe)\n",
    "    print(f'CTPE - Training Loss: {train_loss_ctpe:.4f}')\n",
    "    \n",
    "    # Evaluate Baseline\n",
    "    val_loss_baseline, val_acc_baseline, val_f1_baseline = evaluate_baseline(model_baseline, val_loader_baseline)\n",
    "    print(f'Baseline - Validation Loss: {val_loss_baseline:.4f}, Accuracy: {val_acc_baseline:.4f}, F1 Score: {val_f1_baseline:.4f}')\n",
    "    \n",
    "    # Evaluate CTPE\n",
    "    val_loss_ctpe, val_acc_ctpe, val_f1_ctpe = evaluate_ctpe(model_ctpe, val_loader_ctpe)\n",
    "    print(f'CTPE - Validation Loss: {val_loss_ctpe:.4f}, Accuracy: {val_acc_ctpe:.4f}, F1 Score: {val_f1_ctpe:.4f}')\n",
    "\n",
    "# Function to compute accuracy for Baseline (Bi-Encoder)\n",
    "def compute_accuracy_baseline(model, dataloader, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Computes accuracy for the Baseline (Bi-Encoder) model.\n",
    "    Args:\n",
    "        model: BaselineDocumentSimilarityModel instance.\n",
    "        dataloader: DataLoader for the validation data.\n",
    "        threshold: Threshold for classifying similarity scores.\n",
    "    Returns:\n",
    "        Computed accuracy.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_examples = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Accuracy Evaluation Baseline'):\n",
    "            # Move data to GPU\n",
    "            title_input_ids = batch['title_input_ids'].to(device)\n",
    "            title_attention_mask = batch['title_attention_mask'].to(device)\n",
    "            abstract_input_ids = batch['abstract_input_ids'].to(device)\n",
    "            abstract_attention_mask = batch['abstract_attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            similarities = model(\n",
    "                title_input_ids,\n",
    "                title_attention_mask,\n",
    "                abstract_input_ids,\n",
    "                abstract_attention_mask\n",
    "            )\n",
    "            \n",
    "            # Compute accuracy\n",
    "            preds = (similarities >= threshold).float()\n",
    "            correct = (preds == labels).float().sum()\n",
    "            total_correct += correct.item()\n",
    "            total_examples += len(labels)\n",
    "    \n",
    "    accuracy = total_correct / total_examples\n",
    "    return accuracy\n",
    "\n",
    "# Function to compute accuracy for CTPE (Cross-Encoder)\n",
    "def compute_accuracy_ctpe(model, dataloader, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Computes accuracy for the CTPE (Cross-Encoder) model.\n",
    "    Args:\n",
    "        model: CTPEDocumentSimilarityModel instance.\n",
    "        dataloader: DataLoader for the validation data.\n",
    "        threshold: Threshold for classifying similarity scores.\n",
    "    Returns:\n",
    "        Computed accuracy.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_examples = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Accuracy Evaluation CTPE'):\n",
    "            # Move data to GPU\n",
    "            titles = batch['title']\n",
    "            abstracts = batch['abstract']\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Tokenize the concatenated title and abstract\n",
    "            encodings = tokenize_texts_ctpe(titles, abstracts)\n",
    "            input_ids = encodings['input_ids'].to(device)\n",
    "            attention_mask = encodings['attention_mask'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            # Compute predictions\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs >= threshold).float()\n",
    "            correct = (preds == labels).float().sum()\n",
    "            total_correct += correct.item()\n",
    "            total_examples += len(labels)\n",
    "    \n",
    "    accuracy = total_correct / total_examples\n",
    "    return accuracy\n",
    "\n",
    "# Final Evaluation\n",
    "final_accuracy_baseline = compute_accuracy_baseline(model_baseline, val_loader_baseline)\n",
    "final_accuracy_ctpe = compute_accuracy_ctpe(model_ctpe, val_loader_ctpe)\n",
    "\n",
    "print(f'\\nFinal Validation Accuracy:')\n",
    "print(f'Baseline (Bi-Encoder): {final_accuracy_baseline:.4f}')\n",
    "print(f'CTPE (Cross-Encoder): {final_accuracy_ctpe:.4f}')\n",
    "\n",
    "# Function to get embeddings for Baseline (Bi-Encoder)\n",
    "def get_embeddings_baseline(model, dataloader):\n",
    "    \"\"\"\n",
    "    Extracts embeddings for titles and abstracts using the Baseline (Bi-Encoder) model.\n",
    "    Args:\n",
    "        model: BaselineDocumentSimilarityModel instance.\n",
    "        dataloader: DataLoader for the validation data.\n",
    "    Returns:\n",
    "        Tuple of numpy arrays containing title embeddings and abstract embeddings.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    title_embeddings = []\n",
    "    abstract_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Extracting Baseline Embeddings'):\n",
    "            # Titles\n",
    "            title_input_ids = batch['title_input_ids'].to(device)\n",
    "            title_attention_mask = batch['title_attention_mask'].to(device)\n",
    "            title_outputs = model.minilm(\n",
    "                input_ids=title_input_ids,\n",
    "                attention_mask=title_attention_mask\n",
    "            )\n",
    "            title_pooled_output = model.dropout(title_outputs.last_hidden_state[:, 0, :]).cpu().numpy()\n",
    "            title_embeddings.append(title_pooled_output)\n",
    "            \n",
    "            # Abstracts\n",
    "            abstract_input_ids = batch['abstract_input_ids'].to(device)\n",
    "            abstract_attention_mask = batch['abstract_attention_mask'].to(device)\n",
    "            abstract_outputs = model.minilm(\n",
    "                input_ids=abstract_input_ids,\n",
    "                attention_mask=abstract_attention_mask\n",
    "            )\n",
    "            abstract_pooled_output = model.dropout(abstract_outputs.last_hidden_state[:, 0, :]).cpu().numpy()\n",
    "            abstract_embeddings.append(abstract_pooled_output)\n",
    "    \n",
    "    title_embeddings = np.concatenate(title_embeddings, axis=0)\n",
    "    abstract_embeddings = np.concatenate(abstract_embeddings, axis=0)\n",
    "    return title_embeddings, abstract_embeddings\n",
    "\n",
    "# Function to get embeddings for CTPE (Cross-Encoder)\n",
    "def get_embeddings_ctpe(model, dataloader):\n",
    "    \"\"\"\n",
    "    Extracts embeddings for title-abstract pairs using the CTPE (Cross-Encoder) model.\n",
    "    Args:\n",
    "        model: CTPEDocumentSimilarityModel instance.\n",
    "        dataloader: DataLoader for the validation data.\n",
    "    Returns:\n",
    "        Numpy array containing pair embeddings.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    pair_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Extracting CTPE Embeddings'):\n",
    "            titles = batch['title']\n",
    "            abstracts = batch['abstract']\n",
    "            encodings = tokenize_texts_ctpe(titles, abstracts)\n",
    "            input_ids = encodings['input_ids'].to(device)\n",
    "            attention_mask = encodings['attention_mask'].to(device)\n",
    "            outputs = model.minilm(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            pooled_output = model.dropout(outputs.last_hidden_state[:, 0, :]).cpu().numpy()\n",
    "            pair_embeddings.append(pooled_output)\n",
    "    \n",
    "    pair_embeddings = np.concatenate(pair_embeddings, axis=0)\n",
    "    return pair_embeddings\n",
    "\n",
    "# Extract Embeddings\n",
    "print(\"\\nExtracting Embeddings for Baseline (Bi-Encoder)...\")\n",
    "title_embeddings_baseline, abstract_embeddings_baseline = get_embeddings_baseline(model_baseline, val_loader_baseline)\n",
    "print(f'Title Embeddings Shape (Baseline): {title_embeddings_baseline.shape}')\n",
    "print(f'Abstract Embeddings Shape (Baseline): {abstract_embeddings_baseline.shape}')\n",
    "\n",
    "print(\"\\nExtracting Embeddings for CTPE (Cross-Encoder)...\")\n",
    "pair_embeddings_ctpe = get_embeddings_ctpe(model_ctpe, val_loader_ctpe)\n",
    "print(f'Pair Embeddings Shape (CTPE): {pair_embeddings_ctpe.shape}')\n",
    "\n",
    "# Compute per-sample cosine similarities\n",
    "def compute_per_sample_cosine_similarity(title_embeds, abstract_embeds):\n",
    "    \"\"\"\n",
    "    Computes cosine similarity for each pair of title and abstract embeddings.\n",
    "    Args:\n",
    "        title_embeds (np.ndarray): Embeddings for titles, shape (n_samples, hidden_size)\n",
    "        abstract_embeds (np.ndarray): Embeddings for abstracts, shape (n_samples, hidden_size)\n",
    "    Returns:\n",
    "        np.ndarray: Array of cosine similarity scores, shape (n_samples,)\n",
    "    \"\"\"\n",
    "    # Normalize embeddings\n",
    "    title_norm = title_embeds / np.linalg.norm(title_embeds, axis=1, keepdims=True)\n",
    "    abstract_norm = abstract_embeds / np.linalg.norm(abstract_embeds, axis=1, keepdims=True)\n",
    "    \n",
    "    # Compute element-wise cosine similarity\n",
    "    cosine_sim = np.sum(title_norm * abstract_norm, axis=1)\n",
    "    return cosine_sim\n",
    "\n",
    "# Compute per-sample similarities for the validation set\n",
    "similarities_baseline_correct = compute_per_sample_cosine_similarity(title_embeddings_baseline, abstract_embeddings_baseline)\n",
    "\n",
    "\n",
    "# Compute Similarity Scores for CTPE\n",
    "# CTPE already models the similarity, using sigmoid-activated logits as similarity scores\n",
    "print(\"\\nUsing CTPE logits as similarity scores...\")\n",
    "similarity_scores_ctpe = torch.sigmoid(torch.tensor(pair_embeddings_ctpe)).numpy()\n",
    "print(f'Similarity Scores (CTPE): {similarity_scores_ctpe[:5]}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
